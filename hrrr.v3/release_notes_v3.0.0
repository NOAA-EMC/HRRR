Release Notes:  HRRR v3.0.0


v3.0.0 - released February 14, 2018
* Now uses WRF Version 3.8.1
* Updated versions of GSI and post processor
* Introduce HRRR Alaska (run every third hour)
* Forecasts extended to 36 hours at 00/06/12/18z
* Change to hybrid vertical coordinate
* Improved microphysics, LSM, and PBL schemes
* Refined roughness lengths over various land use types
* Gives more ensemble weight to hybrid DA
* Assimilation of lightning data, radar radial velocities, AMVs over land, and TAMDAR
* METAR and GOES cloud building made consistent
* Upgrade being made concurrently with RAPv4


REPOSITORY DETAILS
     After cloning the EMC_hrrr git repository, retrieve the new code from the hrrr.v3 directory.
     The build script is located here: EMC_hrrr/hrrr.v3/sorc/build_hrrr.scr


 Information on the brand new HRRR-Alaska
* Only run every third hour
* Uses the RAP files for initial and boundary conditions, but uses the awp242bgrb files


Output and Resources
* output changes - conus 
   * handful of new parameters in all output files
   * output now available to f36 at 00/06/12/18z
   * com output now located in conus subdirectory within .../com/hrrr/prod/hrrr.$YYYYMMDD
   * pcom output is now located in ../com/hrrr/prod/hrrr.$YYYYMMDD/conus/wmo
   * nawips output is now located in .../com/hrrr/prod/hrrr.$YYYYMMDD/conus/nawips


* output - alaska 
   * complete new set of Alaska products
   * output available to f18 at 03/09/15/21z and to f36 at 00/06/12/18z
   * com output located in alaska subdirectory within .../com/hrrr/prod/hrrr.$YYYYMMDD
   * pcom output is located in .../com/hrrr/prod/hrrr.$YYYYMMDD/alaska/wmo
   * nawips output is located in .../com/hrrr/prod/hrrr.$YYYYMMDD/alaska/nawips


* compute resource information - conus
   * still runs every hour, but now to f36 at 00/06/12/18z
   * forecast job changes from 93 nodes (24 tasks/node) to 108 nodes (24 tasks/node)
   * analysis job changes from 10 nodes (24 tasks/node) to 15 nodes (24 tasks/node)
   * post job changes from 2 nodes at 16 tasks/node to 2 nodes at 24 tasks/node[a]
   * single cycle runtime for non-extended forecasts seems to increase by a couple of minutes but WCOSS Cray timings have been very inconsistent
      * runtime changes from 32 to ~34-36 minutes for forecast job
   * single cycle runtime for extended forecasts increases by ~40 minutes but WCOSS Cray timings have been very inconsistent 
      * runtime increases from 32 minutes to ~65 minutes for forecast job and from 22 minutes to 28 minutes for makebc job
   * disk space usage changes 
      * Total: 2.75 TB/day to 3.3 TB/day
      * com: 2.4 TB/day to 2.8 TB/day
      * nawips subdirectory: 310 GB/day to 365 GB/day
      * wmo subdirectory: 58 GB/day to 72 GB/day


* compute resource information - alaska
   * runs every third hour, out to f18 at 03/09/15/21z and out to f36 at 00/06/12/18z
   *    * disk space usage
      * Total: 0.8 TB/day
      * com: 0.6 TB/day
      * nawips subdirectory: 212 GB/day
      * wmo subdirectory: 11 GB/day


* Data retention for files in /com and /nwges under prod/para/test environments
   * asking to maintain current retention of data in /com and in prod and recommend same retention for parallel.
   * Please mirror smartinit, wrfprs, wrfsfc, wrfnat, class1 bufr files to development machine along with nawips subdirectories


* new executables
   * hrrr_process_lightning
   * hrrr_smartinit_ak
   * hrrr_update_gvf


* revised executables (conus and alaska will use same executables except for smartinit)
   * hrrr_full_cycle_surface
   * hrrr_gsi
   * hrrr_process_cloud
   * hrrr_process_enkf
   * hrrr_process_imssnow
   * hrrr_process_mosaic
   * hrrr_process_sst
   * hrrr_ref2tten
   * hrrr_smartinit_conus
   * hrrr_sndp
   * hrrr_stnmlist
   * hrrr_update_bc
   * hrrr_wps_metgrid - formerly hrrr_metgrid
   * hrrr_wps_ungrib - formerly hrrr_ungrib
   * hrrr_wrfarw_fcst - formerly hrrr_wrfarw
   * hrrr_wrfarw_real - formerly hrrr_real
   * hrrr_wrfbufr
   * hrrr_wrfpost
     
* eliminated executables
   * hrrr_wgrib2




* changes to directories
   * fix, parm, scripts, ush - conus / alaska subdirectories must be created
   * removal of util directory (no longer needed; the parm cards are now located in parm/wmo)




* timing changes
   * For 00/06/12/18z cycles, the makebc job must be initiated ~20 minutes earlier to account for processing of extra forecast hours needed for extensions
   * Alaska HRRR jobs should be kicked off at same time as CONUS HRRR jobs




* pre-implementation testing requirements
   * need to test RTMA/URMA upgrade concurrently
   * need to test NARRE-TL upgrade concurrently
   * need to test obsproc upgrade concurrently
   * need to test verification code changes
   * need to test smartinit update
   * please get guesses for conus and alaska for the hrrrges_sfc directories from the developers
   * need to run parallel RAP simultaneously - HRRRv3 cannot run without RAPv4 running


* Dissemination info
   * output should be placed into /gpfs/hps/nco/ops/com/hrrr/para
   * output should be placed on ftp server
   * output should be placed on paranomads - a directory structure has been set up
   * request that all gempak output be transferred to DEVWCOSS as well as all wrfsfc files
   * code is proprietary, and restricted data is used but is not disseminated


* Archive to HPSS
   * scripts may need to be modified to save extra forecast hours at 00/06/12/18z
   * Add HRRR Alaska to HPSS archive - please set up hpss retention scripts to save the same files being saved for the CONUS HRRR
   * Estimates for tarballs for runhistory
      * bufr: 0.768 GB/day to 1.58 GB/day
      * init: 1.34 TB/day to 1.64 TB/day
      * wrf: 240 GB/day to 254 GB/day


[a]This may not be worth noting here but I wasn't sure so I left it in.